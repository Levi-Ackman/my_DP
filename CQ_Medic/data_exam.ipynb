{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.layers import TransformerTimeSeriesForecast\n",
    "import torch.optim as optim \n",
    "from torch import nn\n",
    "from utils.auxiliary_fn import train_one_ep,evaluate_one_ep\n",
    "from models.LSTM_FCN import LSTM_FCN_2_Atten\n",
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda')\n",
    "else :\n",
    "    device=torch.device('cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_val_test import h_d_train,h_d_val,h_d_test,get_h_f_dataloader,get_data\n",
    "\n",
    "span_len=60\n",
    "pre_len=1\n",
    "batch_size=32\n",
    "\n",
    "train_dataset=h_d_train(cut_len=span_len,slide_win_size=pre_len,transform=False)\n",
    "val_dataset=h_d_val(cut_len=span_len,slide_win_size=pre_len)\n",
    "test_dataset=h_d_test(cut_len=span_len,slide_win_size=pre_len)\n",
    "\n",
    "\n",
    "train_dataloader=get_h_f_dataloader(train_dataset,batch_size,0)\n",
    "val_dataloader=get_h_f_dataloader(val_dataset,batch_size,0)\n",
    "test_dataloader=get_h_f_dataloader(test_dataset,batch_size,0)\n",
    "\n",
    "x,y=next(iter(train_dataloader))\n",
    "# print(x.shape)\n",
    "# print(y.shape)\n",
    "\n",
    "print(x[30,29,:])\n",
    "print(y[29])\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, targets, sequence_length):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Select the time series starting from index\n",
    "        time_series = self.data[index:index+self.sequence_length]\n",
    "\n",
    "        # Return the time series and the target value after the series\n",
    "        return time_series, self.targets[index+self.sequence_length-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of time series in the dataset\n",
    "        return len(self.data) - self.sequence_length + 1\n",
    "\n",
    "# Generate some random time series data\n",
    "time_series_data = np.random.randn(1000, 2)\n",
    "\n",
    "# Generate some random target values\n",
    "targets = np.random.randint(0, 10, size=(1000,))\n",
    "\n",
    "# Create a time series dataset\n",
    "dataset = TimeSeriesDataset(torch.tensor(time_series_data,dtype=torch.float32), torch.tensor(targets,dtype=torch.float32), sequence_length=60)\n",
    "\n",
    "# Create a dataloader for the dataset\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=next(iter(dataloader))\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # self.fc2= nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc1(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "'''定义模型和优化器'''\n",
    "# model = TransformerTimeSeriesForecast(\n",
    "#     feature_dim=2,\n",
    "#     pre_len=pre_len,\n",
    "#     num_heads=2,\n",
    "#     hidden_dim=64,\n",
    "#     num_layers=10,\n",
    "#     data_len=span_len\n",
    "#     )\n",
    "\n",
    "model = LSTMNet(input_dim=2, hidden_dim=32, output_dim=1, num_layers=1).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0015)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.00015,betas=(0.9,0.999))\n",
    "# 定义损失函数\n",
    "# criterion = nn.MSELoss()\n",
    "from utils.loss_fn.TildeQ_loss import TildeQLoss\n",
    "from utils.loss_fn.standard_loss import mase_loss,huber_loss,log_cosh_loss,pinball_loss,wmape_loss,smape_loss\n",
    "criterion=mase_loss\n",
    "\n",
    "# 训练循环\n",
    "num_epochs=20000\n",
    "epoch_loss=1000000000\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0  # 定义一个变量用于累计当前 epoch 的总损失\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 反向传播，更新参数\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 累计当前 epoch 的总损失\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # # 打印训练状态\n",
    "        # if (i + 1) % 10 == 0:\n",
    "        #     print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, len(train_dataloader), loss.item()))\n",
    "\n",
    "    # 打印当前 epoch 的平均损失\n",
    "    if epoch_loss>(running_loss / len(train_dataloader.dataset)):\n",
    "        epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "        print('Epoch [{}/{}], Average Loss: {:.4f}'.format(epoch + 1, num_epochs, epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TransformerTimeSeriesForecast(\n",
    "#     feature_dim=7,\n",
    "#     pre_len=pre_len,\n",
    "#     num_heads=8,\n",
    "#     hidden_dim=256,\n",
    "#     num_layers=10,\n",
    "#     data_len=span_len\n",
    "#     )\n",
    "\n",
    "model=LSTM_FCN_2_Atten(\n",
    "    data_len=span_len,\n",
    "    pre_len=pre_len,\n",
    "    input_dim=1,\n",
    "    hidden_dim=8,\n",
    "    num_layers=8\n",
    ")\n",
    "\n",
    "learning_rate=0.0015\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "# Define the loss function\n",
    "# loss_fn = nn.SmoothL1Loss()  ## optional_1\n",
    "\n",
    "from utils.loss_fn.soft_dtw import SoftDTW\n",
    "# Create the \"criterion\" object\n",
    "loss_fn = SoftDTW(use_cuda=False, gamma=0.1)\n",
    "\n",
    "\n",
    "# Create the data loader\n",
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda')\n",
    "else :\n",
    "    device=torch.device('cpu')\n",
    "# Train the model\n",
    "\n",
    "num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, criterion, optimizer, train_loader, evaluate_loader,val_loader,train_fn,test_fn,device,num_epochs):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a PyTorch model for time series forecasting.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model to train and evaluate.\n",
    "        criterion: The loss function to use for training and evaluation.\n",
    "        optimizer: The optimizer to use for training.\n",
    "        train_loader (torch.utils.data.DataLoader): The data loader to use for training.\n",
    "        val_loader (torch.utils.data.DataLoader): The data loader to use for evaluation.\n",
    "        num_epochs (int): The number of epochs to train the model for.\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_train_loss=train_fn(optimizer, criterion, train_loader, model,device)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Evaluation loop\n",
    "        model.eval()\n",
    "        val_loss = test_fn(criterion, val_loader, model,device)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Final evaluation on validation set\n",
    "    model.eval()\n",
    "    val_loss = test_fn(criterion, evaluate_loader, model,device)\n",
    "    print(f\"Final Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_and_evaluate(\n",
    "#     model=model,\n",
    "#     criterion=loss_fn,\n",
    "#     optimizer=optimizer,\n",
    "#     train_loader=train_dataloader,\n",
    "#     val_loader=val_dataloader,\n",
    "#     evaluate_loader=test_dataloader,\n",
    "#     train_fn=train_one_ep,\n",
    "#     test_fn=evaluate_one_ep,\n",
    "#     device=device,\n",
    "#     num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
